{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from os import path\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import ldaseqmodel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.read_pickle(\"./tweets_per_weeks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = datetime.date(2019,3,31)\n",
    "to_date = datetime.date(2020,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datelist = pd.date_range(from_date, to_date, freq='2w').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datelist = pd.date_range(from_date, to_date, freq='2w').tolist()\n",
    "#data= dict()\n",
    "#col_names =  ['weeks', 'date', 'tweet']\n",
    "#my_df  = pd.DataFrame(columns = col_names)\n",
    "for i in range(0,26) :\n",
    "    if i in [5,10,15]:\n",
    "        time.sleep(5)\n",
    "    text_query = 'Insurance Canada'\n",
    "    since_date = str(datelist[i].date())\n",
    "    until_date = str(datelist[i+1].date())\n",
    "    #count = 1000\n",
    "    #Creation of query object\n",
    "    tweetCriteria = manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date)#.setMaxTweets(count)\n",
    "    # Creation of list that contains all tweets.\n",
    "    tweets = manager.TweetManager.getTweets(tweetCriteria)\n",
    "    for tweet in tweets : \n",
    "        my_df.loc[len(my_df)] = [\"weeks_\"+str(i+1), tweet.date.date(), tweet.text]\n",
    "#      # Creating list of chosen tweet data\n",
    "#     #text_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
    "#     my_df.loc[len(my_df)] = [\"weeks_\"+str(i+1), tweet.date, tweet.text] for tweet in tweets]\n",
    "\n",
    "#     print(\"weeks_\"+str(i+1))\n",
    "#     #data[\"weeks_\"+str(i+1)] = [tweet.text for tweet in tweets]\n",
    "\n",
    "my_df.to_pickle(\"./tweets_per_weeks.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_df.tweet.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\"¯\\_\" ,\"ツ\", \"_/¯\",\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod ):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def preprocess_tweets(tweets ):\n",
    "    # convert text to lower-case\n",
    "    tweets = [tweet.lower() for tweet in tweets]\n",
    "    \n",
    "    # remove URLs\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'http\\S+', '', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove emojis from tweet\n",
    "    tweets = [emoji_pattern.sub(r'', tweet) for tweet in tweets]\n",
    "    \n",
    "    # Remove non-alphabet\n",
    "    tweets = [re.sub(r'[^a-zA-Z]|(\\w+:\\/\\/\\S+)',' ', tweet) for tweet in tweets]\n",
    " \n",
    "    # tokenization\n",
    "    tweets_word_tokens = [word_tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    # Remove short words (length < 3)\n",
    "    tweets_word_tokens = [[w for w in tweet_word_tokens if len(w)>2] for tweet_word_tokens in tweets_word_tokens]\n",
    "    \n",
    "    # remove stopwords\n",
    "    additional_stop_words=['j']\n",
    "    stop_words = set(stopwords.words('english')).union(additional_stop_words)\n",
    "    tweets_word_tokens_nostops = [[word for word in tweet if ( word not in stop_words and word not in emoticons and word not in string.punctuation)] for tweet in tweets_word_tokens]\n",
    "\n",
    "       # Unigram\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(tweets_word_tokens, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    # form bigram\n",
    "    tweets_tokens_bigrams = make_bigrams(tweets_word_tokens_nostops, bigram_mod )\n",
    "    \n",
    "    return tweets_tokens_bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preprocess\n",
    "data_clean = preprocess_tweets(data) \n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_clean)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_clean=pd.read_pickle(\"./clean_tweets_per_weeks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = my_df_clean.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dur = {}\n",
    "# LDA models\n",
    "for i in range(24,102,2):\n",
    "        start = time.time()\n",
    "        lda_model = LdaMulticore(corpus,id2word=id2word, num_topics=i,passes=50, iterations =1000, workers = 3)\n",
    "        end = time.time()\n",
    "        dict_dur[str(i)+'topic'] = end-start\n",
    "        lda_model.save(path.join('output','tweets_Lda_model_' + str(i) + \"_topics\"))\n",
    "        del lda_model\n",
    "        ps=pd.DataFrame([dict_dur])\n",
    "        ps.to_pickle(\"./dict_dur.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models=[]\n",
    "for i in range(2,102,2):\n",
    "    lda_models.append(gensim.models.LdaModel.load(\"output/tweets_Lda_model_\"+str(i)+\"_topics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lda_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(lda_models[2].print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perplexity=[]\n",
    "Coherence=[]\n",
    "for i in range(50):\n",
    "    # Compute Perplexity\n",
    "    Perplexity.append(lda_models[i].log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "    # Compute coherence\n",
    "    Coherence.append(CoherenceModel(model=lda_models[i], texts=data_clean, dictionary=id2word, coherence='c_v').get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(Perplexity).to_pickle(\"./Perplexity.pkl\")\n",
    "\n",
    "pd.DataFrame(Coherence).to_pickle(\"./Coherence.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Show graph\n",
    "limit=102; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, Coherence)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models[17].num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=102; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, Perplexity)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"LogPerplexity score\")\n",
    "plt.legend((\"Perplexity_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic LDA // k = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice=list(numpy.asarray(my_df['weeks'].value_counts()[my_df['weeks'].unique()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ldaseq = time.time()\n",
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=id2word, time_slice=time_slice, num_topics=36, chunksize=1, chain_variance=0.05)\n",
    "end_ldaseq = time.time()\n",
    "dur_ldaseq =end_ldaseq-start_ldaseq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaseq.save(path.join('output',\"tweets_Dynamic_Lda_model_36_topics\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaSeqModel\n",
    "\n",
    "ldaseq = LdaSeqModel.load('./output/tweets_Dynamic_Lda_model_36_topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the distribution of words of a fixed topic over time \n",
    "topic_term_matrix=dict()\n",
    "for pas in range(26):\n",
    "    topic_term_matrix[\"time\"+str(pas)]= ldaseq.print_topic(17, time=pas, top_terms=34833)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the distribution over time of a chosen word from the topic_term_matrix\n",
    "dental1=list()\n",
    "# covid= list()\n",
    "# dental2=list()\n",
    "#cancellation=list()\n",
    "for key in topic_term_matrix:\n",
    "    for tupels in topic_term_matrix[key]:\n",
    "        if (\"dental\" ) in tupels:\n",
    "            dental1.append(tupels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dental1_beta=list()\n",
    "for c in dental1:\n",
    "    dental1_beta.append(c[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates= [\"2019-03-31\",\"2019-04-14\",\"2019-04-28\",\"2019-05-12\",\"2019-05-26\",\"2019-06-09\",\"2019-06-24\",\"2019-07-07\",\"2019-07-21\",\"2019-08-04\",\"2019-08-18\",\"2019-09-01\",\"2019-09-15\"\n",
    "        ,\"2019-09-29\",\"2019-10-13\",\"2019-10-27\",\"2019-11-10\",\"2019-11-24\",\"2019-12-08\",\"2019-12-22\",\"2020-01-05\",\"2020-01-19\",\"2020-02-02\",\"2020-02-16\",\"2020-03-01\",\"2020-03-15\"]\n",
    "\n",
    "df3=pd.DataFrame([dental1_beta])\n",
    "# Change the column names \n",
    "df3.columns =dates\n",
    "  \n",
    "# Change the row indexes \n",
    "df3.index = [\"dental\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.transpose().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaseq.print_topic_times(topic=22)[25] # evolution of topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document topic distribution\n",
    "Doc_topic_matrix=dict()\n",
    "for doc_num in range(len(my_df)):\n",
    "    Doc_topic_matrix[\"doc\"+str(doc_num)]=list(ldaseq.doc_topics(doc_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  ['topic'+str(i) for i in range(36)]\n",
    "Doc_topic_matrix = pd.DataFrame(Doc_topic_matrix,index=col_names)\n",
    "Doc_topic_matrix = Doc_topic_matrix.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_topic_matrix.to_pickle(\"./Doc_topic_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_topic_matrix = pd.read_pickle(\"./Doc_topic_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most representative doc of a given topic\n",
    "topic_num = 17\n",
    "Doc_topic_matrix[Doc_topic_matrix['topic'+str(topic_num)]==Doc_topic_matrix['topic'+str(topic_num)].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['tweet'][174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_topic_matrix[Doc_topic_matrix['topic19']==Doc_topic_matrix['topic19'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sentence  for sentence in set(list(my_df[\"tweet\"])) if 'universal' in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_topic_matrix_by_time= dict()\n",
    "pos=0\n",
    "ind=0\n",
    "for num in time_slice:\n",
    "    Doc_topic_matrix_by_time[\"corpus\"+str(ind)]=Doc_topic_matrix[pos:pos+num]\n",
    "    pos= pos+num\n",
    "    ind=ind+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most representative doc of a given topic by fortnight\n",
    "\n",
    "topic_num = 3\n",
    "time_stamp=19\n",
    "Doc_topic_matrix_by_time[\"corpus\"+str(time_stamp)][Doc_topic_matrix_by_time[\"corpus\"+str(time_stamp)]['topic'+str(topic_num)]==Doc_topic_matrix_by_time[\"corpus\"+str(time_stamp)]['topic'+str(topic_num)].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['tweet'][28823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame()\n",
    "for key in Doc_topic_matrix_by_time:\n",
    "#     print(len(pd.DataFrame(dic[key].mean())))\n",
    "#     df[key]= list(pd.DataFrame(dic[key].mean()))\n",
    "    df = pd.concat([df, pd.DataFrame(Doc_topic_matrix_by_time[key].mean())], axis=1)\n",
    "df.columns =dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max\"] = df.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df['max']<0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = df[df2]\n",
    "second = df[-df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "figure = plt.gcf() # get current figure\n",
    "figure.set_size_inches(8, 6)\n",
    "\n",
    "first.iloc[:,:-1].transpose().plot(title=\"evolution of topics prorpotions \")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks(rotation=90)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
